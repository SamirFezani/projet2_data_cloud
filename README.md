# ğŸš€ Projet 2 â€” Collecte et Stockage de DonnÃ©es sur le Cloud

## ğŸŒŸ Objectif

Mettre en place une infrastructure **ETL complÃ¨te** permettant de :

* Extraire des offres d'emploi via **scraping HTML** et **API (Remotive.io ou PÃ´le Emploi)**.
* Transformer et nettoyer les donnÃ©es collectÃ©es.
* Stocker les donnÃ©es localement et dans le **cloud (AWS S3 ou RDS)**.
* Visualiser les rÃ©sultats via un **dashboard interactif (Plotly/Dash)**.

---

## ğŸ§± Pipeline ETL

### ğŸ“… Extraction (`etl/`)

* `extract.py` : Scraping dâ€™un site (ex: Weworkremotely, RemoteOK, ou HTML local).
* `api_remotive.py` : RÃ©cupÃ©ration dâ€™offres via API Remotive.io.
* `api_pe.py` *(optionnel)* : RequÃªte Ã  lâ€™API PÃ´le Emploi.

### ğŸ”„ Transformation (`transform.py`)

* Nettoyage des intitulÃ©s, entreprises et lieux.
* Formatage JSON pour stockage.

### ğŸ“‚ Chargement (`load.py`)

* Lecture/Ã©criture locale des fichiers `.json` dans `data/processed/`.
* Optionnel : Upload vers **Amazon S3**.

---

## ğŸ“Š Visualisation (`dashboard/`)

Dashboard interactif avec \[Plotly Dash] affichant :

* Histogramme des lieux les plus frÃ©quents.
* RÃ©partition des offres par entreprise.
* Nuage de mots des titres de poste.

---

## â˜ï¸ Cloud (Facultatif)

### âœ… AWS S3

* Stockage des fichiers JSON transformÃ©s.
* AccÃ¨s configurÃ© avec `boto3` et `aws configure`.

### âœ… AWS RDS (PostgreSQL)

* *(Optionnel)* Connexion possible via `psycopg2` pour charger les offres dans une base de donnÃ©es PostgreSQL sur RDS.

---

## ğŸ“ Arborescence du projet

```
projet2_data_cloud/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â””â”€â”€ processed/
â”œâ”€â”€ etl/
â”‚   â”œâ”€â”€ extract.py
â”‚   â”œâ”€â”€ transform.py
â”‚   â”œâ”€â”€ load.py
â”‚   â”œâ”€â”€ api_remotive.py
â”‚   â””â”€â”€ api_pe.py
â”œâ”€â”€ dashboard/
â”‚   â””â”€â”€ dashboard.py
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ exploration.ipynb
â”œâ”€â”€ infra/
â”‚   â”œâ”€â”€ s3_config.md
â”‚   â”œâ”€â”€ rds_config.md
â”‚   â””â”€â”€ iam_roles.md
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

---

## âš™ï¸ Installation

1. Cloner le dÃ©pÃ´t :

   ```bash
   git clone https://github.com/SamirFezani/projet2_data_cloud.git
   cd projet2_data_cloud
   ```

2. Installer les dÃ©pendances :

   ```bash
   pip install -r requirements.txt
   ```

---

## â–¶ï¸ ExÃ©cution

1. **Extraction** :

   ```bash
   python etl/extract.py
   ```

2. **Transformation** :

   ```bash
   python etl/transform.py
   ```

3. **Chargement (option AWS)** :

   ```bash
   python etl/load.py
   ```

4. **Dashboard** :

   ```bash
   python dashboard/dashboard.py
   ```

---

## ğŸ“Œ Remarques

* Les fichiers `.html` pour le scraping peuvent Ãªtre stockÃ©s dans `data/raw/`.
* La clÃ© AWS est Ã  configurer via `aws configure`.
* Lâ€™accÃ¨s Ã  lâ€™API PE nÃ©cessite une inscription sur [https://emploi-store.dev](https://emploi-store.dev).

---

## ğŸ™Œ Auteurs

* **Samir Fezani**
* Projet rÃ©alisÃ© dans le cadre du **parcours Data Analyst chez OpenClassrooms**.
